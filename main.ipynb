{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "For any issues running these modules, use python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, add a new field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = yf.Ticker(\"AAPL\")\n",
    "hist = apple.history(start=\"2000-01-01\", end=\"2020-09-30\", actions=False)\n",
    "percent = (hist[\"Close\"]-hist[\"Open\"])/hist[\"Open\"]*100\n",
    "hist[\"%Increase\"] = percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "\n",
    "#Explore dataset\n",
    "hist.head()\n",
    "hist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "\n",
    "Visualize the data that we have extracted to better understand trends to expect and routes to go for changing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Visualization Title')\n",
    "ax1.set_xlabel('xLabel')\n",
    "ax1.set_ylabel('yLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "If we want to transform the data in any way, we can do it here.\n",
    "(Maybe we can transform the date (month/day of the year) to a one-hot array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "labels=[\"01/20\", \"01/21\", \"01/22\"]\n",
    "# Generates an int for each label\n",
    "y=le.fit_transform(labels)\n",
    "\n",
    "# Prints out each date with its int mapping\n",
    "for c in list(le.classes_):\n",
    "    print(le.transform([c])[0], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape data here as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train set and test set\n",
    "train_set_size = 5000\n",
    "data = np.array(hist)\n",
    "train_data = data[:train_set_size]\n",
    "test_data = data[train_set_size:]\n",
    "x_train = data[:train_set_size,:-1]\n",
    "y_train = data[:train_set_size,-1]\n",
    "x_test = data[train_set_size:,:-1]\n",
    "y_test = data[train_set_size:,-1]\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "# We could try this as it might be a bit easier to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(data_to_be_added),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only have experience with keras, so I have to get familiar with pytorch\n",
    "\n",
    "# from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "# from keras.models import Model\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "# inputs = Input(shape=(8000,1))\n",
    "\n",
    "# #First Conv1D layer\n",
    "# conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Second Conv1D layer\n",
    "# conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Third Conv1D layer\n",
    "# conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "# conv = MaxPooling1D(3)(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# #Flatten layer\n",
    "# conv = Flatten()(conv)\n",
    "\n",
    "# #Dense Layer 1\n",
    "# conv = Dense(128, activation='relu')(conv)\n",
    "# conv = Dropout(0.3)(conv)\n",
    "\n",
    "# # Output layer\n",
    "# outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "# model = Model(inputs, outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \n",
    "# mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_tr, y_tr ,epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))\n",
    "\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Visualization\n",
    "\n",
    "Now we can take a look at how successful our model is and can easily find where overfitting takes place (if at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the application part (maybe a separate file), we can use our model to predict the future performance\n",
    "\n",
    "from keras.models import load_model\n",
    "model=load_model('best_model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
